\section{Implementation of the model}
To simulate the Ising model we generated a network using the Phyton library \textbf{Networkx}, then we assigned a random spin, $\pm1$, to each node. Given an initial temperature we thermalized the system using the Metropolis-Hastings algorithm, that we will describe in the next sections. When thermalization is reached we started measuring the quantities we are interested in. This procedure is repeated for a set of different temperatures to obtain the thermodynamic variables and the network measures as functions of the temperature.
\subsection{The Metropolis-Hastings algorithm}
To obtain an equilibrium microstate of a system, monte carlo methods can be used, for our purpose the \textbf{Metropolis-Hastings algorithm} can mimic the thermal fluctuations that are generated by a thermal bath in canonical ensemble.

The main idea behind this algorithm is that, if we have two different states of our system, say $A$ and $B$, and we know the probability of finding the system in each state, say $P(A)$ and $P(B)$, we can connect the two probabilities by the probability of transitioning $A\rightarrow B,\ P(A\rightarrow B)$
\begin{equation*}
    P(B)=P(A\rightarrow B)P(A)\qquad \Leftrightarrow\qquad P(A\rightarrow B)=\frac{P(B)}{P(A)}.
\end{equation*}
In the canonical ensemble the probability of observing the microstate with energy $E_A$ is given by the Boltzmann exponential divided by the canonical partition function, in this way we can easily determine the transition probability between two states of different energies
\begin{equation*}
    P(E_A)=\frac{e^{-\beta E_A}}{Z}\qquad \Rightarrow\qquad P(E_A\rightarrow E_B)=e^{-\beta(E_B-E_A)}.
\end{equation*} 
Note that when $E_B-E_A<0$, the new microstate has a grater energy than the starting one, the probability that the transition occurs is grater then one, in this case we just assume that the probability is one. This could seem a bold assumption, but actually it follows from the idea that equilibrium is reached at minimum of the (free) energy. Our criterion thus become
\begin{equation}
    \label{MHCriterion}
    P(E_A\rightarrow E_B)=\begin{cases}
        1\qquad\qquad\quad\ \  \text{if}\ E_B<E_A\\
        e^{-\beta(E_B-E_A)}\quad \text{if}\ E_B>E_A
    \end{cases}.
\end{equation}
Now, the Metropolis-Hastings algorithm explores the phase space of our system by flipping one random spin at the time, then for each flip we evaluate the variation of the energy, and we decide whether the flip occurs using the criterion \eqref{MHCriterion}. Iterating this procedure, as we already said, we mimic the thermal fluctuation induced by a bath of a given temperature $\beta$, after a big number of iterations the system reaches equilibrium with the bath. The number of iteration needed to reach equilibrium is called \emph{thermalization time} and will vary with the dimension of the phase space that the system has to explore.

A useful feature of this algorithm is that, once reached equilibrium, the system will remain at equilibrium exploring just a neighborhood of the equilibrium microstates. In this way, by continuing the iteration at equilibrium for a sufficient number of steps, in order to suppress the correlations with the first microstate that we found at equilibrium, we can obtain other microstates at equilibrium. These can be then used to measure thermal averages over a fictitious ensemble. The main advantage of this procedure is that the thermalization time is usually grater then the number of steps needed to suppress the correlation between microstates, in this way we can obtain an ensemble in more efficient and faster way than restarting again the whole procedure.

For our simulation the algorithm has been repeated for each temperature, but, since the thermalization time grows with the number of atoms and in theory we would like to reach the thermodynamic limit $N\rightarrow\infty$, we used a trick. If the difference between successive temperatures is small, the "distance" in phase space between the old equilibrium states and the new ones should be small too. For this reason, after the first run at the first temperature, we make evolve the system from the last microstate found to the new temperature, for a number of iteration sufficient to lose correlations between old states and the new ones. In this way the full thermalization has to occur just one time and then each thermalization between each successive temperature is greatly reduced.

\subsection{Measure procedure}
As already mentioned, the Metropolis-Hasings algorithm allows us to efficiently create a canonical ensemble of our system at each temperature. To obtain $N$ samples every $\Delta$ steps (needed to remove correlations) we measured the desired quantities only in the $N\cdot \Delta$ last iterations of the evolution process between successive temperatures, assuming that when we start measuring the system has already reached equilibrium. Then, for each temperature, we averaged over those measures.
\subsubsection{Thermodynamic measures}
Thermodynamic variables are often related one to the other, for this reason some of them cannot be directly measured, but they must be obtained from some that can be measured directly.

We now describe the procedure used to obtain these last ones.
\begin{itemize}
    \item The \textbf{energy} can be evaluated at each step from the Hamiltonian \eqref{Ising_Hamiltonian} (remember that we need to know it for the Metropolis-Hasings algorithm) however this wouldn't be very efficient, since we should go through the whole network and for each atom through its neighbors: for this reason we evaluated it just one time in the beginning, then at each step we calculated the energy contribution of just the flipped spin, since the spin can either be $+1$ or $-1$ the variation of the total energy is twice the energy of the flipped spin.
    \item The \textbf{magnetization} could be evaluated averaging the spins of all the atoms, however again this process can be optimized since the change of magnetization due to a spin flip is just twice its new spin over the number of atoms, for this reason we calculated the full magnetization just at the beginning and the for each step we update it.
    \item The \textbf{entropy} is easily obtained as a function of the magnetization using the micro-
    canonical formula and the Stirling approximation 
    \begin{align*}
        S=\log\omega=\log\frac{N!}{N_+!N_-!}&\simeq N\log N-N-(N_+\log N_+-N_++N_-\log N_--N_-)\\
        &=-N\bigg(\frac{1+m}{2}\log{\frac{1+m}{2}}+\frac{1-m}{2}\log{\frac{1-m}{2}}\bigg).
    \end{align*} 
    \item The \textbf{free energy} is evaluated at each step from its definition $F=E-TS$. 
\end{itemize}
From these we can obtain the \textbf{heat capacity} and the \textbf{magnetization}. Consider the so-called generating function
\begin{equation}
    W(\beta, H)=\log Z_N(\beta, H)=-\beta F(\beta, H),
\end{equation}
recalling the discussion of section 1.1, we can use this function to obtain the above quantities:
\begin{align*}
    NC_V&=\frac{\partial E}{\partial T}= \beta^2\frac{\partial^2\log Z_N}{\partial\beta^2} =\beta^2\frac{\partial^2 W}{\partial\beta^2},\\\quad N\chi&=N\frac{\partial m}{\partial H}=N\frac{\partial}{\partial H}\frac{\left\langle\sigma_i \right\rangle }{N}=\beta\frac{\partial^2 \log Z_N}{\partial H^2}=\beta\frac{\partial^2 W}{\partial H^2}.
\end{align*}
We can calculate the last derivatives of each of the above lines to get 
\begin{align*}
    \frac{\partial^2 W}{\partial\beta^2}&=\frac{\partial^2 \log Z_N}{\partial\beta^2}= \frac{\partial}{\partial\beta}\frac{1}{Z_N}\frac{\partial}{\partial\beta}\sum_{\{\sigma_i\}} \exp(-\beta\mathcal{H} )=-\frac{\partial }{\partial\beta}\frac{1}{Z_N}\sum_{\{\sigma_i\}}\mathcal{H} \exp(-\beta\mathcal{H} )
    \\&=\frac{\sum_{\{\sigma_i\}}\mathcal{H}^2 \exp(-\beta\mathcal{H} )}{Z_N}-\frac{[\sum_{\{\sigma_i\}}\mathcal{H} \exp(-\beta\mathcal{H} )]^2}{Z_N^2}=\left\langle E^2 \right\rangle-\left\langle E \right\rangle^2\\
    \frac{\partial^2 W}{\partial H^2}&=\frac{\partial^2 \log Z_N}{\partial H^2}=\frac{\partial}{\partial H}\frac{1}{Z_N}\frac{\partial}{\partial H}\sum_{\{\sigma_i\}} \exp(-\beta\mathcal{H} )=\frac{\partial}{\partial H}\frac{\beta}{Z_N}\sum_{\{\sigma_i\}}\sum_{i=1}^N \sigma_i \exp(-\beta\mathcal{H} )
    \\&=\frac{\sum_{\{\sigma_i\}}(\sum_{i=0}^N\sigma_i)^2 \exp(-\beta\mathcal{H} )}{Z_N}-\frac{[\sum_{\{\sigma_i\}}\sum_{i=0}^N\sigma_i \exp(-\beta\mathcal{H} )]^2}{Z_N^2}=\left\langle m^2 \right\rangle-\left\langle m \right\rangle^2.
\end{align*}
Putting these results together we discover that the heat capacity and the susceptibility are a measure of the dispersion from the mean of, respectively, the energy and magnetization:
\begin{equation}
    C_V=\frac{\left\langle E^2 \right\rangle-\left\langle E \right\rangle^2}{NT^2},\qquad \chi=\frac{\left\langle m^2 \right\rangle-\left\langle m \right\rangle^2}{NT}.
\end{equation}
These two formulae give us a simple way to measure the heat capacity and the susceptibility, however these must be extracted from the ensemble measures (they need means in the first place) and therefore we haven't measured their average but one single value for each temperature.
\subsubsection{Graph measures}
All the network measures (the quantities of section 1.2.2) are obtained using the Networkx (for more informations \cite{Networkx}) features. For each one we evaluated the average as we have done with the thermodynamic variables.

For each iteration in which a measure occurred we separated the network in two, one containing only \emph{spin up} atoms, and another with only \emph{spin down}, preserving the links of the lattice that connected neighbor aligned spins. Then the measures was conducted on the two networks. As already explained, for some quantities we need connected networks, however this splitting procedure generates also disconnected graphs, for this reason some measures was restricted only on the giant component of each network. Lastly, we compared the results obtained from the spin up and spin down networks.